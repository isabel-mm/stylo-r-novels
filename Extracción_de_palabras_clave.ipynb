{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMchf7dXxTZ7hHUbNBBVrFQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isabel-mm/stylo-r-novels/blob/main/Extracci%C3%B3n_de_palabras_clave.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **C√≥digo para generar 5 palabras clave de cada texto del corpus üòé**\n"
      ],
      "metadata": {
        "id": "XLeJRhWHT1_w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNxfA6wETc7Q",
        "outputId": "4e9eafbf-d025-4486-c678-4192c2fbc692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar modelo de spaCy en la lengua que queramos (en este caso, ingl√©s)\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.max_length=1880820\n",
        "\n",
        "# Directorio que contiene los archivos .txt en Google Drive\n",
        "corpus_dir = '/content/drive/My Drive/Colab Notebooks/ingl√©s'\n",
        "\n",
        "# Lista de nombres de archivo en el directorio\n",
        "file_names = os.listdir(corpus_dir)\n",
        "\n",
        "# Lista para almacenar el contenido de cada archivo\n",
        "corpus = []\n",
        "\n",
        "# Lee cada archivo en el directorio y agrega su contenido a la lista \"corpus\"\n",
        "for file_name in file_names:\n",
        "    with open(os.path.join(corpus_dir, file_name), 'r', encoding=\"latin1\") as f:\n",
        "        corpus.append(f.read())\n",
        "\n",
        "# Procesar el corpus para eliminar nombres propios y nombres de lugares\n",
        "processed_corpus = []\n",
        "for document in corpus:\n",
        "    doc = nlp(document)\n",
        "    # Filtrar los tokens que no son nombres propios ni nombres de lugares\n",
        "    filtered_text = ' '.join(token.text for token in doc if token.ent_type_ not in ('PERSON', 'GPE')) # Etiquetado de entitades en spaCy\n",
        "    processed_corpus.append(filtered_text)\n",
        "\n",
        "# Procesa el corpus con spaCy para la lematizaci√≥n y limpieza de stopwords y caracteres no alfab√©ticos\n",
        "processed_corpus_lemmatized = []\n",
        "for document in processed_corpus:\n",
        "    doc = nlp(document)\n",
        "    processed_text = ' '.join(token.lemma_ for token in doc if not token.is_stop and token.is_alpha)\n",
        "    processed_corpus_lemmatized.append(processed_text)\n",
        "\n",
        "# Creaci√≥n del objeto TfidfVectorizer con los par√°metros deseados\n",
        "vectorizer = TfidfVectorizer(max_df=0.8, min_df=2)\n",
        "\n",
        "# Ajuste y transformaci√≥n del corpus en vectores TF-IDF\n",
        "tfidf_matrix = vectorizer.fit_transform(processed_corpus_lemmatized)\n",
        "\n",
        "# Obtiene los nombres de las caracter√≠sticas\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Funci√≥n para obtener las palabras clave con mayor puntaje TF-IDF de un documento\n",
        "def get_top_tfidf(doc_id, feature_names, tfidf_matrix, top_n=5):\n",
        "    # Obtiene los √≠ndices de las caracter√≠sticas que tienen un valor TF-IDF no nulo en el documento dado\n",
        "    feature_index = tfidf_matrix[doc_id,:].nonzero()[1]\n",
        "    # Obtiene los puntajes TF-IDF correspondientes a las caracter√≠sticas seleccionadas\n",
        "    tfidf_scores = zip(feature_index, [tfidf_matrix[doc_id, x] for x in feature_index])\n",
        "    # Ordena las caracter√≠sticas por puntaje TF-IDF en orden descendente y toma las primeras 'top_n'\n",
        "    top_items = sorted(tfidf_scores, key=lambda x: -x[1])[:top_n]\n",
        "    # Inicializa listas para almacenar las palabras clave y sus puntajes TF-IDF\n",
        "    keywords = []\n",
        "    scores = []\n",
        "    # Recorre los elementos seleccionados y almacena las palabras clave y sus puntajes TF-IDF correspondientes\n",
        "    for item in top_items:\n",
        "        keywords.append(feature_names[item[0]])\n",
        "        scores.append(item[1])\n",
        "    # Devuelve las palabras clave y sus puntajes TF-IDF\n",
        "    return keywords, scores"
      ],
      "metadata": {
        "id": "OX53_6ECTx3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define la ruta completa donde deseas guardar el archivo Excel\n",
        "excel_file_path = '/content/drive/My Drive/Colab Notebooks/palabras_clave.xlsx'\n",
        "\n",
        "# Crea una lista de diccionarios con los nombres de los archivos y sus palabras clave\n",
        "data = []\n",
        "for i, file_name in enumerate(file_names):\n",
        "    keywords, scores = get_top_tfidf(i, feature_names, tfidf_matrix)\n",
        "    if keywords:  # Verifica si keywords no est√° vac√≠o\n",
        "        row = {'Documento': file_name}\n",
        "        for j, keyword in enumerate(keywords):\n",
        "            row[f'Palabra clave {j+1}'] = keyword\n",
        "        data.append(row)\n",
        "\n",
        "# Crea un DataFrame a partir de la lista de diccionarios\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Guarda el DataFrame en un archivo Excel en la ubicaci√≥n especificada\n",
        "df.to_excel(excel_file_path, index=False)\n",
        "\n",
        "print(\"¬°Terminado! Ya tienes el Excel listo :-)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gnz6oZYyV6jZ",
        "outputId": "8140fe81-1e55-4d13-c20c-242a3be8f6ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¬°Terminado! Ya tienes el Excel listo :-)\n"
          ]
        }
      ]
    }
  ]
}